## Exploratory Testing Exercise (Deep Dive): â€œChatbot Tours in Orderlandâ€

You have access to a **simple demo webchat chatbot** embedded in a web page. Check `casanova.py` in `demos` directory.

- It accepts free-text user input.
- Internally it maps each input to exactly one of **three intents**:
  - `TRACK_STATUS`
  - `ASK_ETA`
  - `REPORT_ISSUE`
- For each intent, the chatbot responds with a **stub/simulated answer**:
  - `TRACK_STATUS`: Pretends to look up an order and says something generic.
  - `ASK_ETA`: Pretends to calculate an ETA and gives a generic answer.
  - `REPORT_ISSUE`: Pretends to log an issue and acknowledges it.
- There is **no real backend**; everything is simulated.

Your mission in this exercise is **not** to check â€œif the stub worksâ€, but to **explore how the chatbot behaves around those stubs**: how it classifies, how it handles weird inputs, how it supports (or fails) realistic conversations within those 3 intents.

We will use **exploratory testing tours** to structure our exploration.

---

### 1. Exercise Goal

By the end of this exercise, participants should be able to:

1. **Design and execute exploratory testing tours** tailored to a webchat chatbot.
2. **Reveal bug patterns** from the taxonomy, especially:
   - Intent misclassification,
   - Entity / phrasing issues,
   - Context & conversation flow issues,
   - UX / wording problems.
3. Capture **structured notes** that connect findings to:
   - SDLC stage where they were likely introduced,
   - Component(s) involved,
   - Follow-up test ideas or automation candidates.

---

### 2. Practical Constraints

- Timebox: **45 minutes** total.
- Group size: **2â€“4 testers per group** (if you have multiple groups).
- Each group has:
  - The webchat demo URL.
  - A simple note-taking template (see Section 6).
  - This exercise description.
- Each group explores one tour (25 mins), followed by debrief on interesting findings (~5 mins per group.)
---

### 3. High-Level Test Charter

> **Charter:** Explore how well the chatbot understands and handles user questions about order tracking, ETAs, and reporting issues via webchat, using focused tours to stress different bug patterns (intent classification, phrasing variability, conversation flow, and UX clarity), and identify at least 10 distinct observations (bugs, risks, or questions).

#### Success criteria:

- At least **10 distinct observations** collected, with:
  - 5+ clearly mapped to a **bug pattern category** (you donâ€™t need to be perfect).
  - 2+ ideas for **follow-up automated tests**.
  - 2+ potential **requirements/design gaps** (things missing, not just broken).

---

### 4. Tours

You will perform **four tours**, each 10â€“15 minutes, plus time for note consolidation and debrief.

#### Tour 1 â€“ â€œIntent Safariâ€

**Focus:** How the chatbot **recognizes the correct intent** (or fails to).

**Bug patterns to hunt for:**
- Intent misclassification
- Fallback/irrelevant responses
- Over-confident wrong mappings
- Ambiguous expressions

**Instructions:**

1. Create a list of **natural phrases** real users might use for each intent:

   - `TRACK_STATUS` examples:
     - â€œWhere is my order?â€
     - â€œHas my package shipped?â€
     - â€œCan you check the status of order #12345?â€
   - `ASK_ETA` examples:
     - â€œWhen will my order arrive?â€
     - â€œWhatâ€™s the ETA?â€
     - â€œIs it coming today or tomorrow?â€
   - `REPORT_ISSUE` examples:
     - â€œMy package arrived damaged.â€
     - â€œI got the wrong item.â€
     - â€œI want to report a problem with my order.â€

2. **Gradually distort** these phrases to test boundaries:
   - Add **politeness** and extra words: â€œHey, sorry to bug you, but any idea when my order might show up?â€
   - Add **typos / abbreviations**: â€œwen wil my pakage arive?â€
   - Add **emojis**: â€œMy parcel is late ğŸ˜¡ Where is it?â€
   - Combine multiple intents: â€œMy order is late and also the wrong color. Can I get a new one and when is it coming?â€

3. For each input, note:
   - Which intent you **expected**.
   - What the **chatbot actually did** (if visible).
   - Whether the response **makes sense** for that intent.

4. Specifically try to create **ambiguous** cases:
   - â€œI havenâ€™t received my order yetâ€ (Could be `TRACK_STATUS` or `ASK_ETA`).
   - â€œMy order is missing, what should I do?â€ (could be status, issue, or both).

**Deliverable from this tour:**

- A short table like:

  | Input phrase | Expected intent | Observed behavior | Interesting? (Y/N) | Notes / Bug pattern |
  |-------------|----------------|-------------------|---------------------|---------------------|

Try to find at least **5 interesting misbehaviors**.

---

#### Tour 2 â€“ â€œConversation Mazeâ€

**Focus:** **Multi-turn conversation** behavior (even though responses are stubbed).

**Bug patterns to hunt for:**
- Context loss
- Dialogue loops
- Dead ends
- Incomplete fulfillment (from a userâ€™s point of view)

**Instructions:**

1. Start conversations that **logically should require several turns**, even if the system is stubbed, for example:
   - â€œI want to check the status of my order.â€
   - If the bot doesnâ€™t ask for an order number, **pretend** and provide one: â€œItâ€™s 12345.â€
   - Continue as if you expect a real system: â€œOkay, and if Iâ€™m not at home when it comes, what happens?â€

2. Try sequences such as:
   - `TRACK_STATUS` â†’ follow-up `ASK_ETA` in the same conversation.
   - `ASK_ETA` â†’ user expresses dissatisfaction â†’ `REPORT_ISSUE`.
   - Start with small talk then move into an intent: â€œHi, how are you? Anyway, my orderâ€¦â€.

3. See if the chatbot:
   - Keeps track of **what you just asked**.
   - Behaves consistently when you **refer back** to something:
     - â€œThat one from last week.â€
     - â€œThe order we just talked about.â€

4. Try **breaking the conversation**:
   - Long message, several questions in one.
   - Change topic mid-way (â€œActually, forget tracking, the item is brokenâ€).
   - Rephrase the same question multiple times in different ways.

**Deliverable from this tour:**

- A list of **3â€“5 conversation transcripts** where:
  - You label the **moment the bot seems to lose context or gets weird**.
  - Map those to bug patterns like â€œContext lossâ€ or â€œDead-end conversationâ€.

---

#### Tour 3 â€“ â€œUser Persona Costume Partyâ€

**Focus:** How the chatbot behaves with **different user personas and emotional tones**.

**Bug patterns to hunt for:**
- Poor handling of emotional language.
- Vague or unhelpful responses.
- Hidden UX / wording problems.

**Instructions:**

1. Define at least **3 personas**:

   Example personas:
   - **Angry customer**: â€œMy package is late AGAIN. This is ridiculous.â€
   - **Anxious customer**: â€œIâ€™m really worried, my medicine still hasnâ€™t arrived.â€
   - **Clueless new user**: â€œI donâ€™t understand anything about orders, can you just help me?â€

2. For each persona:
   - Start with **emotionally loaded** but still on-topic messages.
   - See if the chatbot:
     - Acknowledges the problem in a human way, or just responds generically.
     - Provides a **useful direction** (even as a stub).
     - Makes the user **feel heard** (subjectively).

3. Modify messages to be more extreme or more subtle:
   - Add ALL CAPS, exclamation marks, sarcasm, emojis, etc.
   - â€œWhatever, I guess you donâ€™t care if my package is missing ğŸ™„.â€

4. Look for:
   - Responses that feel **tone-deaf** or **inappropriate**.
   - Situations where **small rewordings** change the intent mapping dramatically.

**Deliverable from this tour:**

- A short summary per persona:

  - What phrases you tried.
  - 2â€“3 interesting moments where the chatbot reacted poorly or surprisingly well.
  - Bug/risk tags: e.g. â€œTone not alignedâ€, â€œAmbiguous mappingâ€, â€œPoor guidanceâ€.

---

#### Tour 4 â€“ â€œEdge-of-Reality Tourâ€ (Glitches, Noise & Limits)

**Focus:** Edge cases in **input shape and length**, weirdness, and â€œbreakingâ€ the parser/intent mapping.

**Bug patterns to hunt for:**
- Entity / slot extraction issues
- Input modality handling issues
- Robustness problems

**Instructions:**

1. **Length & complexity:**
   - Very short inputs: â€œStatus?â€, â€œWhen?â€
   - Very long rant: several sentences, multiple concerns.
   - Repeated characters: â€œmyyy orrrderrrâ€.
   - Copy-pasted email or tracking notification text.

2. **Noise & off-topic:**
   - Add bits of unrelated talk: â€œBy the way, your website is super slow, but anyway, where is my order?â€
   - Joke or meme-like phrasing: â€œYo bot, whereâ€™s my stuff at? ğŸ˜…â€

3. **Format weirdness:**
   - Include pseudo order numbers: â€œABC-123-XYZâ€.
   - Mix languages if applicable: â€œWhere is meine Bestellung?â€
   - Use symbols: â€œOrder #12345?!??? Is it coming today?!â€

4. Observe:
   - Does the chatbot still route to a sensible intent?
   - Does it fail in a consistent pattern (helpful for future automation)?
   - Does it get stuck or confuse itself?

**Deliverable from this tour:**

- A list of **â€œglitchesâ€**: inputs that cause clearly weird behavior or misclassification, plus a ranked list of â€œtop 3 weirdest failuresâ€.

---

### 5. Mapping Findings to the Bug Patterns

After the tours, spend **15â€“20 minutes** consolidating:

1. For each interesting observation, try to map it to **a bug pattern** in conversation, integration, UX, etc.

   Example mapping:

   - â€œUser writes â€˜My order is late and also damagedâ€™ â†’ bot treats it only as ASK_ETA and never offers to log an issue.â€
     - Likely pattern: **Intent Misclassification / multi-intent confusion**
     - SDLC stage: **Requirements / Design** (no multi-intent strategy), plus **Implementation** (classifier not trained/configured for multi-intent).
     - Component: **NLU, intent catalog, routing logic**

2. Identify any **gaps or new patterns** not covered before (e.g. purely UX-related).

3. Pick **2â€“3 findings** that:
   - Can be made a part of formal testing.
   - Would make sense to turn into **automated regression tests**.
   - Would require **requirements/design changes** rather than just a bug fix.

---

## 6. Suggested Note-Taking Template

Give each participant/group a simple template like this:

```text
=== Observation ID: OBS-XX ===
Tour: (Intent Safari / Conversation Maze / Persona / Edge-of-Reality)
Input(s) used:
Conversation snippet (if multi-turn):

Expected behavior:

Observed behavior:

Impact (user perspective):

Probable bug pattern (from taxonomy):

Likely SDLC stage introduced:
- [ ] Req
- [ ] Design
- [ ] Implementation
- [ ] Integration
- [ ] Pre-prod test gap
- [ ] Ops / config

Responsible components (guess):
Follow-up ideas:
- Can it be formalized?
- Automation candidate?
- Requirement/UX change?
```