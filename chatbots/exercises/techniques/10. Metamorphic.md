# Deep Testing Exercise: Metamorphic Testing for Conversational Order Tracking Agent

## 1. Scope & Intent

**System under test**

- An AI conversational AI agent that handles **order tracking** across:
  - Channels: **voice (phone)** and **web chat**
  - Regions: **Germany, other EU, US**
  - Languages: **DE / EN**
- The agent:
  - Identifies intent (“Where is my order?”)
  - Authenticates / verifies the user
  - Retrieves order status from backend systems
  - Communicates status and next steps

**Why metamorphic testing here?**

For many conversations, we **don’t have a perfect oracle** (“exactly this sentence is correct”), but we can still test if the system behaves **consistently under transformations** of:

- Wording (paraphrase)
- Channel (voice vs chat)
- Locale (DE vs US, EN vs DE)
- Context (with vs without history)
- Partial / noisy input

Metamorphic relations (MRs) let us generate **follow-up tests** from a seed case and check for **logical consistency** instead of exact text matches. This can reveal bug patterns that are hard to detect with traditional test design (e.g., hidden bias, regional inconsistencies, hallucinated changes).

---

## 2. Targeted Bug Patterns from Taxonomy

We focus this deep test on uncovering patterns (including sub-patterns) such as:

- **Conversation & reasoning**
  - #1 Intent misclassification
  - #2 Entity / slot extraction errors
  - #3 Context loss across turns
  - #6 Incorrect disambiguation
  - #8 Hallucinated / fabricated information
  - #9 Incomplete fulfillment
  - #10 Contradictory responses

- **Integration & data handling**
  - #11 Wrong backend system called
  - #12 Incorrect parameters in API calls
  - #13 Stale / cached data issues
  - #15 Idempotency & duplicate actions
  - #16 Transactionality & rollback failure
  - #17/18 AuthN/AuthZ bugs

- **Internationalization & regional logic**
  - #25 Locale / language mismatch
  - #27 Jurisdiction / regional rule bugs
  - #28 Multi-currency & unit handling bugs

- **ML / prompting**
  - #39 Training data coverage bugs
  - #40 Prompt / instruction design bugs
  - #42 Bias & fairness bugs
  - #43 Drift & environment mismatch bugs

- **Observability / metrics**
  - #45 Metrics miscalculation & attribution bugs

---

## 3. Core Seed Scenario

Baseline seed test (S₀):

> Customer in **Germany** calls via **voice** in **German** and asks for the status of a specific order.

**Preconditions**

- Customer `C1` has:
  - Orders: `O1` (shipped), `O2` (processing), `O3` (cancelled)
- Test environment: stable snapshot; no changes during test batch.
- Backend order system responds deterministically.

**Seed input (S₀)** – canonical version

> *In German, voice*:  
> "Hallo, ich möchte gern wissen, wo meine Bestellung mit der Bestellnummer 123456 ist."

**Seed expected properties (oracle-lite)**

- Intent: `Order_Tracking`
- Entity: `order_id = 123456` → maps to `O1`
- Agent:
  - Authenticates user (if required)
  - Calls correct backend, retrieves status for `O1`
  - Responds with:
    - Correct **status**
    - Correct **estimated delivery date** (if available)
    - No contradiction in follow-up turns

We won’t fix exact wording; we just assert logical properties like *“status matches backend”*, *“no hallucinations”*.

---

## 4. Metamorphic Relations (MRs)

For each MR, we define:

- **Source**: Seed test S₀ (or a previously derived test Sᵢ)
- **Transformation**: How we change input or context
- **Expected invariant (metamorphic relation)**: What must remain consistent or change in a predictable way
- **Unique potential bugs**: What this MR can reveal that traditional single-shot tests might miss

### MR1 – Semantic Paraphrase Invariance (Textual)

**Transformation**

- Generate paraphrased user utterances in the **same language & channel** that are *semantically equivalent*.

Examples (German voice/chat):

- "Können Sie mir den Status meiner Bestellung 123456 sagen?"
- "Wo ist mein Paket mit der Bestellnummer 123456?"
- "Ich möchte den Sendungsstatus für 123456 prüfen."

**Expected invariant**

- Intent **remains** `Order_Tracking`
- Entity `order_id` extracted as `123456`
- Order status + delivery date identical to S₀ (modulo phrasing)
- No systematic shift to fallback or wrong intent

**Unique bugs exposed**

- #1 Intent misclassification for paraphrases not in training data
- #2 Entity extraction only working for “canonical” patterns
- #39 Training data coverage gaps (e.g. "Paket" vs "Bestellung")
- #40 Prompt/config brittle to phrasing

---

### MR2 – Cross-Channel Consistency (Voice ↔ Chat)

**Transformation**

- Convert the **same semantic request** from:
  - Voice → Chat
  - Chat → Voice
- Keep customer, account, backend state identical.

Examples:

- S₁ (voice DE): "Wo ist meine Bestellung 123456?"
- S₂ (chat DE): "Wo ist meine Bestellung 123456?"

**Expected invariant**

- Identical underlying business outcome:
  - Same order chosen (`O1`)
  - Same status, same ETA
- No channel-specific contradictions (e.g., voice says “delivered”, chat says “processing”)
- Handoff from one channel to the other (if supported) must **preserve state** (context continuity test)

**Unique bugs exposed**

- #10 Contradictory responses between channels
- #11 Wrong backend system called (voice uses test, chat uses prod-like, or wrong tenant)
- #25 Locale mismatches dependent on channel config
- #47 Scaling or timeout issues only on one channel causing degraded answers

---

### MR3 – Locale / Region Transformation (DE ↔ US, Same Data)

**Transformation**

- Simulate **same customer, same order data** but for different **regions/locales**:
  - DE customer vs US customer with identical order state in backend test fixtures.
- Use **language + locale-appropriate** wording and numbers.

Examples:

- S₃ (DE, German chat): "Wo ist meine Bestellung 123456?"
- S₄ (US, English chat): "Where is my order 123456?"

**Expected invariant**

- Core facts about the order (status, events timeline) are **consistent** across locales.
- Localized presentation differs in **format only**:
  - Date formats (DD.MM.YYYY vs MM/DD/YYYY)
  - Currency formats (€, $, thousands separators)
- Region-specific rules applied **only where truly jurisdiction-dependent**, not arbitrarily.

**Unique bugs exposed**

- #11 Wrong backend per region (US region mapped to EU backend or vice versa)
- #25 Locale resolver bugs (US customer getting German response)
- #27 Wrong jurisdiction logic applied (e.g. DE cooling-off periods for US)
- #28 Currency formatting, tax info incorrectly transformed

---

### MR4 – Context Enrichment / History Augmentation

**Transformation**

- Start with a **short one-shot query** (no history) vs a **context-rich dialog** where customer and agent have exchanged several turns before asking about the order.

Examples:

- S₀: Single turn: “Where is my order 123456?”
- S₅: Multi-turn:
  1. "Hi, I ordered a TV recently."
  2. Agent asks clarifying questions (time, email, etc.)
  3. Eventually, the user: "It’s the order 123456. Where is it?"

**Expected invariant**

- Final answer about `O1`’s status must be **the same** (status, ETA).
- Context should **help**, not hurt:
  - Agent must not “switch order” or hallucinate different order because of additional context.
  - No loss of previously resolved info (e.g., delivery address, authentication) mid-flow.

**Unique bugs exposed**

- #3 Context loss across turns (history vs one-shot differences)
- #6 Incorrect disambiguation due to older context overriding newer info
- #8 Hallucinations triggered by long context
- #16 Transactionality and state issues in multi-turn flows (partial updates across turns)

---

### MR5 – Authentication Strength Variation

**Transformation**

- For the *same* order and user, vary **authentication strength**, e.g.:
  - S₆: Weak auth (caller ID + partial info)
  - S₇: Strong auth (full security questions passed)

**Expected invariant**

- **Sensitive information** (e.g., full address, tracking URLs with auth tokens, PII) must only appear in **strongly authenticated** scenario.
- Non-sensitive info (high-level status, generic statements) may be available in both.
- Under weaker auth, the answer must be **no more permissive** than under stronger auth. (Monotonicity in access, not the other way around.)

**Unique bugs exposed**

- #17 AuthN bugs (session not tied properly to identity)
- #18 Authorization bugs (over-share under weak auth)
- #29 PII exposure in responses
- #34 Model/prompt data leakage when model ignores auth state

---

### MR6 – Paraphrased Negative / Edge Cases (Non-existent or Different Order)

**Transformation**

- Change the **order reference** while keeping customer and phrasing similar, or use *non-existing order IDs*.
  
Examples:

- S₈: Valid order "123456"
- S₉: Non-existing "123457"
- S₁₀: Order belonging to another customer "999999"

All phrased similarly: “Where is my order 123457?” etc.

**Expected invariant**

- For non-existing or foreign orders:
  - Agent **must not** return real status of someone else’s order.
  - Agent must provide safe fallback (e.g., “I can’t find that order”) and/or offer human escalation.
- No order status should be fabricated for non-existent orders.

**Unique bugs exposed**

- #2 Entity extraction mis-generalizing to nearest ID
- #8 Hallucinated / fabricated order information
- #18 Authorization bugs (data from another customer)
- #29 PII exposure from foreign orders

---

### MR7 – Time-Shifted World (Temporal Metamorphism)

**Transformation**

- Execute the same conversation at **different simulated points in time**, while order’s backend state evolves according to test fixture rules.

Example setup:

- T₀: Order `O1` = “processing”, ETA = in 3 days
- T₁ (T₀ + 2 days): `O1` = “shipped”, ETA = tomorrow
- T₂ (T₀ + 5 days): `O1` = “delivered”, history includes delivery event

Run the same question at T₀, T₁, T₂:

- “Where is order 123456?”

**Expected invariant**

- Agent answers must align with **time-dependent backend state**:
  - No regression to older status.
  - No contradictory statuses across time that violate backend event sequence.

**Unique bugs exposed**

- #13 Stale / cached data issues (answers not updating over time)
- #43 Environment mismatch & drift (agent logic not updated after process change)
- #47 Latency / outdated replicas in specific regions

---

### MR8 – ASR / Noise Transformations (Voice-specific)

**Transformation**

- Take one semantic voice query, generate:
  - Clean speech
  - Noisy speech (background noise)
  - Different accents
  - Slower / faster speech
- Use **ASR output** as the input to the NLU and conversational logic.

**Expected invariant**

- For intelligible variants:
  - Intent must remain `Order_Tracking`.
  - Entity extraction robust enough for typical accent/noise.
- Degradation should be **graceful**:
  - Low-confidence → safe clarifications / escalation, not hallucination.

**Unique bugs exposed**

- #19 ASR mapping errors
- #1 Intent misclassification triggered by ASR errors
- #42 Bias & fairness (performance difference by accent)

---

### MR9 – Backend Variation (Multiple Orders, Prioritization Rules)

**Transformation**

- Keep user and wording constant but modify **backend data configuration**:
  - Only one recent order
  - Multiple orders (old + new)
  - Multiple orders with same item but different dates

Examples:

- S₁₁: One order (`O4`) within last week
- S₁₂: Two orders, `O4` (last week), `O5` (today)
- S₁₃: Two orders, one cancelled, one shipped

Same query: “Where is my order?” (no ID given).

**Expected invariant**

- Agent’s decision rule for picking which order to show must be **consistent** and documentable (e.g., most recent, or ask disambiguation).
- Changing backend config must change behavior in **predictable** way (more orders → disambiguation, not random pick).

**Unique bugs exposed**

- #6 Incorrect disambiguation
- #11 Wrong backend dataset for some configurations
- #37 Policy interpretation errors if there are rules about which order to talk about

---

### MR10 – Metric-level Metamorphism (Analytics Sanity)

**Transformation**

- Use all above MRs to generate **sets of conversations** where we know aggregate properties (e.g., same number of successful conversations across channels/locales by construction).

Example:

- For each of 3 locales, run the same seed + metamorphic transformations for 100 customers with identical backend states.

**Expected invariant**

- Aggregate metrics (containment, success rate, average handling time) must:
  - Be consistent with the **synthetic ground truth** (if we design the batches).
  - Not show impossible values (e.g., >100% containment)
  - Show predictable patterns: paraphrases vs canonical forms should not systematically degrade unless intentionally configured.

**Unique bugs exposed**

- #45 Metrics miscalculation & attribution bugs
- #42 Bias & fairness (systematically worse metrics for certain language/region due to hidden bugs revealed by transformation)
- #50 Testing gaps & orchestration (if tests are not reflected in metrics at all)

---

## 5. Execution Outline

### 5.1 Steps

1. **Define seed set** of canonical order-tracking queries S₀ for:
   - 3 regions (DE, EU, US)
   - 2 channels (voice, chat)
   - 2–3 user archetypes per region

2. For each seed, generate follow-up tests using MRs 1–9:
   - Paraphrases
   - Channel switches
   - Locale switches
   - Context-rich versions
   - Auth variations
   - Non-existing/foreign orders
   - Time-shifted runs
   - ASR/noise variants
   - Backend data variations

3. **Run in a controlled test environment**, capturing:
   - Conversation transcripts
   - Backend calls
   - Auth decisions
   - Metrics (success/containment, errors)

4. **Evaluate metamorphic relations**, not exact wording:
   - Build checkers that compare:
     - Status equality / inequality where expected
     - Data consistency across variants
     - Safety properties (no extra info under weak auth, no PII leak)
     - Monotonicity / invariants under transformations

### 5.2 Why this is uniquely powerful

Compared to traditional techniques (equivalence partitioning, boundary value, decision tables), metamorphic testing here surfaces:

- **Cross-channel contradictions** that no single test case can see.
- **Bias and fairness** issues through systematic variation (accent, locale) where no explicit oracle exists.
- **Drift and stale data** issues over time by explicit temporal transformations.
- **Auth/PII consistency** across different authentication strengths.
- **Hidden integration misconfigurations** (wrong tenant, wrong backend) only visible when comparing logically equivalent requests across regions/channels.

These are *emergent* bugs, often invisible when you inspect tests in isolation but obvious under metamorphic comparisons.

---
